TODO WEEK 5:

0. Teacher-forcing on transformers and find out if it's actually doign that
<!-- 1. See if LSTM also has lower validation loss -->
1. Validate both transformers and LSTM with CRPS and see which one is better
<!-- 3. Try out Autoformer and informer and see if it's better -->
2. See if you can train with other metrics like CRPS or KL Div
<!-- 4. Figure out how to deal with empty values (0.0) for some of the columns -->
3. Implement early stopping
4. Maybe implement the teacher forcing starting with only 1 patch
      <!-- 0. Read up all the divergence thing -->
      <!-- 1. Read up CRPS -->
   <!-- 6. use CRPS for validation, see if I can change the loss function to someting similar -->
   <!-- 3. Use electric dataset  -->
   <!-- 1. Try adding random noise as an independent feature to LSTM -->
   <!-- 2. Implement train val test lines function -->
   <!-- 3. Add random state and Try hyperparameter tuning X -->
   <!-- 6. Try differnet Informer and Autoformer X -->
   <!-- 4. Try different prediction lengths  -->
5. Try different granularity
6. Try differetn patch sizes
<!-- 8. Find different datasets of multiple households -->

7. LSTM with all feature with refeeding
8. LSTM with Higher window size
   <!-- 3. Transfomer with no refeeding -->
   <!-- 11. Transformer with refeeding -->
9. Transformer with refeeding with all feature
10. Transformer with refeeding with higher or smaller window
11. Compute differences between the MSE for 1 step ahead, 2 step, 3 step ahead

12. Organize the functions

- Differences between MSE for refeeding and no refeeding
