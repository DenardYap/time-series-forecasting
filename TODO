WEEK 7 UPDATE:

<!--
1. Got aldous to working and started training. Wasn't able to use too big of a hyperparameter since
   that doesn't fit into memory -->

<!-- 1. https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html -->
<!-- Remove the outliers -->

<!-- Train the new model with no outliers -->

<!-- Separate the mean and apply the KL formula (only look at the covariance terms) - 30 mins -->
<!-- Try l-infinity - 30 mins -->

<!-- 0. Look for bugs in code -->

<!-- 1. Try different prediction length to see where KL Div breaks down -> Try both separating and not separating -->

<!-- 2. Augment data by adding gaussian with very small variance -->

<!-- 3. Find out the smallest positive eigenvalues (e.g. 10), add an identity matrix that has a coef that is much smaller than the eigenvalues -->

<!-- 4. Implement early stopping -->

1. Read more papers
2. Test out informer's codes! and see if you can get any close to the results

<!-- 1. Before scale/after scale? prediction length -->
<!-- 1. Use KL Divergence loss - will be very slow, try a smalelr subset -->

Updates Qs:

<!--
1. Turns out I did have a scaling bug in my code, I fixed it and able to achieve 4 - 5 times lower KL divergence

2. Augmented data by adding gaussian noise of mean 0 and std dev 0.1, should I make them larger?

3. Previously when I was adding gaussian noise to LSTM, I add it as a separate dimension, but for this one we are adding it
   to the value itself, what's the difference between those two?

4. starts deviating!

5. Adding noise added some improvements, not sure if it matters -->

<!-- Why use l-infinity? -->

<!-- 1. Separate into mean and cov matrices and see which one is contributing to the high loss
1. Redo separation code -->
   <!-- - Is there a way where I can disconnect and the script keeps running? -->
   <!-- - Is this GPU shared across all people? -->

<!-- 0. Try jensen shannon -->

<!-- 1. Try L1 for the difference between the forecast and prediction and then plot -->
<!-- 2. Find mu and cov for each prediction in group (of 10), then compte the difference between the mu and cov
   between the true and pred
   find out the kl divergence between them -->

<!-- 2. Try to find out why the KL divergence is really high -->

<!-- - Bug in code -->

<!-- 3. Start using the aldous GPU -->

   <!-- 2. Read research papers and see what metrics people used. Find another metric that's standardized. -->
   <!-- 3. Proofread codes -->

<!-- - Try different ways to extract the pdf -->
<!-- - Look at the histogram and see if there's any udnerlyign distribution -->

<!-- 2. Move things to the cloud -->

<!-- 3. Restrict values to positive (doing gelu or smtg) -->
<!-- 4. Dig into how to modify the loss functions of informer -->
<!-- 0. Teacher-forcing on transformers and find out if it's actually doign that -->

5. Transformer try to predict the whole time-series with 1 patch and decide the windowsize for that
<!-- 1. See if LSTM also has lower validation loss -->
6. Validate both transformers and LSTM with CRPS and see which one is better
<!-- 3. Try out Autoformer and informer and see if it's better -->
7. See if you can train with other metrics like CRPS or KL Div
<!-- 4. Figure out how to deal with empty values (0.0) for some of the columns -->
8. Implement early stopping
9. Maybe implement the teacher forcing starting with only 1 patch
      <!-- 0. Read up all the divergence thing -->
      <!-- 1. Read up CRPS -->
   <!-- 6. use CRPS for validation, see if I can change the loss function to someting similar -->
   <!-- 3. Use electric dataset  -->
   <!-- 1. Try adding random noise as an independent feature to LSTM -->
   <!-- 2. Implement train val test lines function -->
   <!-- 3. Add random state and Try hyperparameter tuning X -->
   <!-- 6. Try differnet Informer and Autoformer X -->
   <!-- 4. Try different prediction lengths  -->
10. Try different granularity
11. Try differetn patch sizes
<!-- 8. Find different datasets of multiple households -->

12. LSTM with all feature with refeeding
13. LSTM with Higher window size
    <!-- 3. Transfomer with no refeeding -->
    <!-- 11. Transformer with refeeding -->
14. Transformer with refeeding with all feature
15. Transformer with refeeding with higher or smaller window
16. Compute differences between the MSE for 1 step ahead, 2 step, 3 step ahead

17. Organize the functions

- Differences between MSE for refeeding and no refeeding
